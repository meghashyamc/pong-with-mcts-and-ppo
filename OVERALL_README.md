# Overall Readme

## AIM

The aim of the project is to compare the training performance of Proximal Policy Optimization (PPO)
with PPO-MCTS (PPO combined with Monte Carlo Tree Search (MCTS)) and to compare the playing
performance of Monte Carlo Tree Search (MCTS) with MCTS aided by pre-trained PPO in playing
Simple and Complex Pong games.

## OVERALL METHODOLOGY

The methodology and the experiments conducted are shown in the file `Model/complete_methodolody_flowchart.jpg`.

## DATA GENERATION

No dataset is used in this project. All data that is compared was generated by experiments
that were carried out in the project. These experiments involved training an agent to play Simple/Complex Pong
using PPO and PPO-MCTS, and playing Simple/Complex Pong using MCTS and MCTS aided by pre-trained PPO.
Then the performance of all these algorithms was compared.

## MODEL

The complete process flow of how PPO is trained to play Simple/Complex Pong is shown
in the file `Model/ppo_model_process_flow.png`.

The change in the process flow when PPO is modified to PPO-MCTS and trained to play Simple/Complex Pong is shown in the file `Model/changes_in_ppo-mcts_model_process_flow.png`. The changes in PPO to convert it into
PPO-MCTS have been somewhat inspired by prior research but are completely original in their specific implementation.

The complete process flow of how MCTS plays Simple/Complex Pong is shown in the file `Model/mcts_model_process_flow.png`.

The change in the process flow when MCTS aided by pre-trained PPO plays Simple/Complex Pong is shown
in the file `Model/changes_in_mcts_aided_by_ppo_model_process_flow.png`. The changes in MCTS to convert
it into MCTS aided by pre-trained PPO are a mix of inspiration from prior research and original, specific implementation changes.

## CODE

All the source code for Simple and Complex Pong game implementations, PPO, PPO-MCTS, MCTS and MCTS aided by PPO is in the `code/src` folder.

The folder `code/src/algorithms` contains implementations of all four algorithms.
The folder `code/src/pong` contains implementations of Simple/Complex Pong games.
The folder `code/src/train` contains a script to train an agent (using PPO/PPO-MCTS) to play Simple/Complex Pong.
The folder `code/src/play` contains a script to play (using MCTS/MCTS aided by PPO) Simple/Complex Pong.

The file `code/CODE_README.md` explains how to run the code from scratch and replicate all experiments.

Trained algorithm files (`.pth` files) for PPO and PPO-MCTS training sessions are in the `code/trained` folder. The folder `code/trained/ComplexPong` contains trained files for playing Complex Pong and the folder `code/trained/SimplePong` contains trained files for playing Simple Pong.
Trained algorithm files start with the prefix `PPO_frequent`, `PPO_sparse`, `PPO_mct_frequent` or `PPO_mcts_sparse`. These prefixes indicate whether they were trained with frequent or sparse rewards and whether it was only PPO that was trained or PPO-MCTS that was trained.

All results that were obtained during training (PPO/PPO-MCTS) and playing  (MCTS/MCTS aided by pre-trained PPO) are in the `code/results` folder. The sub-folders inside that folder indicate the specifics of which
results they represent. The 'raw' data that is generated during training and playing is combined using three Python scripts that are also part of the results folder. The script `code/results/max_episode_calculator.py` generates a csv file listing the number of episodes needed for each training session. The script `code/results/time_taken_calculator.py` generates a csv file listing the total time taken for each training session. The script `code/results/training_average_calculator.py` generates a csv file that contains the moving average of paddle hits for each episode across all training sessions.
